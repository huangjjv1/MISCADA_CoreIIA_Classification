
# Have a general look on data
```{r}
library("data.table")
library("mlr3verse")
library("rsample")
library("skimr")
skim(adult)
# pairs(adult)
DataExplorer::plot_histogram(adult, ncol = 3)
DataExplorer::plot_boxplot(adult, by = "salary", ncol = 3)
DataExplorer::plot_intro(adult)
DataExplorer::plot_correlation(adult)
```
# To do the basic data analysis
```{r}
adult_tr = adult
adult_tr$salary<-ifelse(adult_tr$salary==" <=50K",0,1)
library(caret)
## Loading required package: lattice
## Loading required package: ggplot2

dmy<-dummyVars("~.",data=adult_tr)
adultsTrsf<-data.frame(predict(dmy,newdata=adult_tr))
dim(adult_tr)
dim(adultsTrsf)
head(adultsTrsf)
str(adultsTrsf)
cor.prob<-function(X,dfr=nrow(X)-2){
  R<-cor(X,use="pairwise.complete.obs")
  above<-row(R)<col(R)
  r2<-R[above]^2
  Fstat<-r2*dfr/(1-r2)
  R[above]<-1-pf(Fstat,1,dfr)
  R[row(R)==col(R)]<-NA
  R
}

flattenSquareMatrix<-function(m){

  if((class(m) !="matrix") | (nrow(m) !=ncol(m))) stop("Must be asquare matrix.")
  if(!identical(rownames(m),colnames(m))) stop("Row and column names must be equal.")
  ut<-upper.tri(m)
  data.frame(i=rownames(m)[row(m)[ut]],
             j=rownames(m)[col(m)[ut]],
             cor=t(m)[ut],
             p=m[ut])
}
corMasterList<-flattenSquareMatrix(cor.prob(adultsTrsf))

dim(corMasterList)
corList<-corMasterList[order(-abs(corMasterList$cor)),]
head(corList)
selectedSub<-subset(corList,(abs(cor)>0.2 & j =="salary"))
bestSub<-as.character(selectedSub$i[c(1,3,5,6,8,9)])
library(psych)
pairs.panels(adultsTrsf[c(bestSub,"salary")])

```
#KNN part
```{r}
# data splitting
# adult$salary<-ifelse(adult$salary==" <=50K",0,1)
# adult_test$salary<-ifelse(adult_test$salary==" <=50K",0,1)
adult_knn = adult
adult_test_knn = adult_test
adult_knn[,label := rep("train",nrow(adult_knn))]
adult_test_knn[,label := rep("test",nrow(adult_test_knn))]
adult_total_data <- rbind(adult_knn,adult_test_knn,fill=  TRUE)
str(adult_total_data)

library(dplyr)
library(caret)
adult_total_data_part <- select(adult_total_data,workclass,marital_status,occupation,relationship,race,sex,native_country) 
dummies <- dummyVars(~., data = adult_total_data_part, levelsOnly = FALSE, fullRank = TRUE)
adult_total_data_unordered <- predict(dummies, newdata = adult_total_data_part) %>% as.data.frame()

adult_total_data_part2 <- select(adult_total_data,age,education_num,capital_gain,capital_loss,hours_per_week)
adult_total_data_numerical <- preProcess(adult_total_data_part2,method = "scale") %>% predict(.,adult_total_data_part2)

library(stringr)
adult_total_data_part3 <- select(adult_total_data,salary,label)
adult_total_data_part3[,salary := gsub("[\\. ]","",salary)]
adult_total_data_target <-adult_total_data_part3[,salary := factor(salary,levels = c("<=50K",">50K"),labels = c(0,1))]


adult_total_data_final <- cbind(adult_total_data_target,adult_total_data_unordered,adult_total_data_numerical)
adult_train_knn <- adult_total_data_final[label == "train",] %>% select(.,-label)
adult_test_knn <- adult_total_data_final[label == "test",] %>% select(.,-label)
library(class)
library(pROC)

find_k_rows= 1000
#firstly use 10 to begin with
adult_pre  <- knn(train = adult_train_knn[1:find_k_rows,-1],test = adult_test_knn[1:find_k_rows,-1],cl = adult_train_knn$salary[1:find_k_rows],k= 1)
auc <- auc(as.numeric(adult_test_knn[1:find_k_rows]$salary),as.numeric(adult_pre))
auc
# to find the best k value here
temp = 0

for (i in 1:20){
  adult_pre  <- knn(train = adult_train_knn[1:find_k_rows,-1],test = adult_test_knn[1:find_k_rows,-1],cl = adult_train_knn$salary[1:find_k_rows],k= i)
  auc <- auc(as.numeric(adult_test_knn[1:find_k_rows]$salary),as.numeric(adult_pre))
  if(auc >temp) {temp = auc;temp_k = i}
}
temp
temp_k
plot(roc(as.numeric(adult_test_knn$salary[1:find_k_rows]),as.numeric(adult_pre)))
auc_knn <- auc(as.numeric(adult_test_knn$salary[1:find_k_rows]),as.numeric(adult_pre))
print(paste("The best k is",temp_k,"and the auc value is:",temp))
```
# DeepLearning part
# obtaining set
```{r}
# adult$salary<-ifelse(adult$salary==" <=50K",0,1)
# adult_test$salary<-ifelse(adult_test$salary==" <=50K."," <=50K"," >50K")
library("data.table")
library("mlr3verse")
library("rsample")
# First get the validate
adult_split <- initial_split(adult)
adult_split2 <- initial_split(testing(adult_split), 0.5)
adult_validate <- training(adult_split2)
```

```{r}
library("recipes")

cake <- recipe(salary ~ ., data = adult) %>%
  step_meanimpute(all_numeric()) %>% # impute missings on numeric values with the mean
  step_center(all_numeric()) %>% # center by subtracting the mean from all numeric features
  step_scale(all_numeric()) %>% # scale by dividing by the standard deviation on all numeric features
  step_unknown(all_nominal(), -all_outcomes()) %>% # create a new factor level called "unknown" to account for NAs in factors, except for the outcome (response can't be NA)
  step_dummy(all_nominal(), one_hot = TRUE) %>% # turn all factors into a one-hot coding
  prep(training = adult) # learn all the parameters of preprocessing on the training data

adult_train_final <- bake(cake, new_data = adult) # apply preprocessing to training data
adult_validate_final <- bake(cake, new_data = adult_validate) # apply preprocessing to validation data
adult_test_final <- bake(cake, new_data = adult_test) # apply preprocessing to testing data
```

```{r}
library("keras")
adult_train_x <- adult_train_final %>%
  select(-starts_with("salary_")) %>%
  as.matrix()
adult_train_y <- adult_train_final %>%
  select("salary_X..50K") %>%
  as.matrix()

adult_test_x <- adult_test_final %>%
  select(-starts_with("salary_")) %>%
  as.matrix()
adult_test_y <- adult_test_final %>%
  select("salary_X..50K") %>%
  as.matrix()

adult_validate_x <- adult_validate_final %>%
  select(-starts_with("salary_")) %>%
  as.matrix()
adult_validate_y <- adult_validate_final %>%
  select("salary_X..50K") %>%
  as.matrix()

```

```{r}
head(adult_test_y,10)
```


```{r}
deep.net <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu",
              input_shape = c(ncol(adult_train_x))) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
# Have a look at it
deep.net
```
# fit the net
```{r}
deep.net %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(),
  metrics = c("accuracy")
)

deep.net %>% fit(
  adult_train_x, adult_train_y,
  epochs = 20, batch_size = 50,
  validation_data = list(adult_validate_x, adult_validate_y),
)
```

```{r}
# To get the probability predictions on the test set:
pred_test_prob <- deep.net %>% predict_proba(adult_test_x)

# To get the raw classes (assuming 0.5 cutoff):
pred_test_res <- deep.net %>% predict_classes(adult_test_x)

auc_dnn <- auc(as.numeric(adult_test$salary),as.numeric(pred_test_res))
auc_dnn
table(pred_test_res, adult_test_y)
yardstick::accuracy_vec(as.factor(adult_test_y),
                        as.factor(pred_test_res))
yardstick::roc_auc_vec(as.factor(adult_test_y),
                       c(pred_test_prob))
```
# randomForest Part
# Training randomForest model
```{r}
library("recipes")

cake <- recipe(salary ~ ., data = adult) %>%
  step_meanimpute(all_numeric()) %>% # impute missings on numeric values with the mean
  step_center(all_numeric()) %>% # center by subtracting the mean from all numeric features
  step_scale(all_numeric()) %>% # scale by dividing by the standard deviation on all numeric features
  step_unknown(all_nominal(), -all_outcomes()) %>% # create a new factor level called "unknown" to account for NAs in factors, except for the outcome (response can't be NA)
  step_dummy(all_nominal(), one_hot = TRUE) %>% # turn all factors into a one-hot coding
  prep(training = adult) # learn all the parameters of preprocessing on the training data

adult_split <- initial_split(adult)
adult_split2 <- initial_split(testing(adult_split), 0.5)
adult_validate <- training(adult_split2)

adult_train_final <- bake(cake, new_data = adult) # apply preprocessing to training data
adult_validate_final <- bake(cake, new_data = adult_validate) # apply preprocessing to validation data
adult_test_final <- bake(cake, new_data = adult_test) # apply preprocessing to testing data

adult_task <- TaskClassif$new(id = "salary",
                               backend = adult, # <- NB: no na.omit() this time
                               target = "salary",
                               positive = ' >50K')
cv5 <- rsmp("cv", folds = 5)
cv5$instantiate(adult_task)
lrn_baseline <- lrn("classif.featureless", predict_type = "prob")
lrn_cart <- lrn("classif.rpart", predict_type = "prob")
res_baseline <- resample(adult_task, lrn_baseline, cv5, store_models = TRUE)
res_cart <- resample(adult_task, lrn_cart, cv5, store_models = TRUE)
res_baseline$aggregate()
res_cart$aggregate()
res <- benchmark(data.table(
  task       = list(adult_task),
  learner    = list(lrn_baseline,
                    lrn_cart),
  resampling = list(cv5)
), store_models = TRUE)
res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.auc"),
                   msr("classif.fpr"),
                   msr("classif.fnr")))
trees <- res$resample_result(2)
tree1 <- trees$learners[[1]]
tree1_rpart <- tree1$model
plot(tree1_rpart, compress = TRUE, margin = 0.1)
text(tree1_rpart, use.n = TRUE, cex = 0.8)
plot(res$resample_result(2)$learners[[5]]$model, compress = TRUE, margin = 0.1)
text(res$resample_result(2)$learners[[5]]$model, use.n = TRUE, cex = 0.8)
lrn_cart_cv <- lrn("classif.rpart", predict_type = "prob", xval = 10)

res_cart_cv <- resample(adult_task, lrn_cart_cv, cv5, store_models = TRUE)
rpart::plotcp(res_cart_cv$learners[[5]]$model)
lrn_cart_cp <- lrn("classif.rpart", predict_type = "prob", cp = 0.016)
res <- benchmark(data.table(
  task       = list(adult_task),
  learner    = list(lrn_baseline,
                    lrn_cart,
                    lrn_cart_cp),
  resampling = list(cv5)
), store_models = TRUE)

res$aggregate(list(msr("classif.ce"),
                   msr("classif.acc"),
                   msr("classif.auc"),
                   msr("classif.fpr"),
                   msr("classif.fnr")))

library("randomForest")
n<-length(names(adult))    
rate=c()    #error rate list
print(paste("Times is:",n-1))
# To find the bet mtry
# for(i in 1:(n-1)){
#   rf_train<-randomForest(as.factor(adult$salary)~.,data=adult,mtry=i,ntree=1000,na.action = na.roughfix)
#   rate[i]<-mean(rf_train$err.rate)   #calculate all the error rate based on OOB model
#   # print(rf_train)
# }
# plot(rate)
# mtry best is 3
rf_train<-randomForest(as.factor(adult$salary)~.,data=adult,mtry = 3,ntree=1000 ,na.action = na.roughfix)
# ?randomForest
importance<-randomForest::importance(rf_train,type = 2)
(importance)
hist(treesize(rf_train))
barplot(rf_train$importance[,1],main="Importance of Input vaiables",cex.names = 0.65)
plot(rf_train)
pie(rf_train$importance[,1])



```
#Prediction
```{r}
library("data.table")
library("mlr3verse")
library("rsample")
library(pROC)
pred<-predict(rf_train,newdata=adult_test)  
pred_out_1<-predict(object=rf_train,newdata=adult_test,type="prob")
table(pred,adult_test$salary) 
auc_rf <- auc(as.numeric(adult_test$salary),as.numeric(pred))
auc_rf
sum(diag(table))/sum(table)  #accuracy
```

